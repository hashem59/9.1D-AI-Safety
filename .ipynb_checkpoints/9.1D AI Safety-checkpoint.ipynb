{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Fj2O3Au9xdS"
   },
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#  Copyright (c) 2023. Mohamed Reda Bouadjenek, Deakin University              +\n",
    "#           Email:  reda.bouadjenek@deakin.edu.au                              +\n",
    "#                                                                              +\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");             +\n",
    "#   you may not use this file except in compliance with the License.           +\n",
    "#    You may obtain a copy of the License at:                                  +\n",
    "#                                                                              +\n",
    "#                 http://www.apache.org/licenses/LICENSE-2.0                   +\n",
    "#                                                                              +\n",
    "#    Unless required by applicable law or agreed to in writing, software       +\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,         +\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  +\n",
    "#    See the License for the specific language governing permissions and       +\n",
    "#    limitations under the License.                                            +\n",
    "#                                                                              +\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to your assignment this week! \n",
    "\n",
    "\n",
    "To deepen our comprehension of safety in AI, this assignment delves into a practical scenario involving **Image Classification** for traffic signs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/0_SkE_FOCh8qGP9Zqh.png\" width=\"450\"/> \n",
    "\n",
    "In the realm of artificial intelligence, ensuring safety stands as a paramount concern, particularly when the technology is harnessed for critical applications like autonomous driving vehicles. One example is the recognition of traffic signs, where the consequences of inaccurate predictions can reverberate dramatically, potentially resulting in tragic loss of life. In light of these profound implications, this assignment delves into the imperative topic of safety in AI, centering on the vital context of image classification for traffic signals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Through an examination of safety within the realm of traffic sign classification, our objective is to unveil and mitigate potential risks inherent in such systems. This endeavor is driven by the pursuit of a more profound comprehension of how AI algorithms can inadvertently introduce risks and uncertainties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the packages you will need for this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbbWh-959k8g"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score, classification_report\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.python.keras.saving import hdf5_format\n",
    "from keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "import h5py, itertools, collections\n",
    "import itertools\n",
    "from tqdm import trange\n",
    "\n",
    "##################\n",
    "# Verifications:\n",
    "#################\n",
    "print('GPU is used.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'GPU is NOT used.')\n",
    "print(\"Tensorflow version: \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "We use a traffic sign dataset that is composed of 48 folders (one for each sign) with 20-500 images in each folder. The total number of images is about 10,000.\n",
    "\n",
    "For reading these images, we use `DirectoryIterator` in `tf.keras.preprocessing.image` that is an iterator capable of reading images from a directory on disk and is capable to extract labels. We also use `ImageDataGenerator` to split this dataset into training and validation set, this later is used to tune the hyperparameters of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpgROocq9k8k"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Split train and validation.\n",
    "'''\n",
    "# We define the size of input images to 64x64 pixels.\n",
    "image_size = (64, 64)\n",
    "# We define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create an image generator with a fraction of 40% images reserved for validation:\n",
    "image_generator = ImageDataGenerator(validation_split=0.4)\n",
    "\n",
    "# Now, we create a training data iterator by creating batchs of images of the same size as \n",
    "# defined previously, i.e., each image is resized in a 64x64 pixels format.\n",
    "train_ds =  DirectoryIterator(\n",
    "    \"traffic_sign_dataset/\",\n",
    "    image_generator,\n",
    "    class_mode='categorical',\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    subset = 'training',\n",
    ")\n",
    "\n",
    "# Similarly, we create a validation data iterator by creating batchs of images of the same size as \n",
    "# defined previously, i.e., each image is resized in a 64x64 pixels format.\n",
    "val_ds = DirectoryIterator(\n",
    "    \"traffic_sign_dataset/\",\n",
    "    image_generator,\n",
    "    class_mode='categorical',\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    subset = 'validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# We save the list of classes (labels).\n",
    "class_names = list(train_ds.class_indices.keys())\n",
    "\n",
    "# We also save the number of labels.\n",
    "num_classes = train_ds.num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "\n",
    "Now, we do data exploration to show you samples of the images and their labels and some statistics to help you in understanding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjIlDsQW9k8m"
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "#### Show distribution of images per class.\n",
    "###############################################\n",
    "counter=collections.Counter(train_ds.labels)\n",
    "v = [ [class_names[item[0]],item[1]]  for item in counter.items()]\n",
    "df = pd.DataFrame(data=v, columns=['index','value'])\n",
    "g = sns.catplot(x='index', y= 'value',  data=df, kind='bar', \n",
    "                legend=False,height=4,aspect=4,saturation=1)\n",
    "(g.despine(top=False,right=False))\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"#images\")\n",
    "plt.title(\"Distribution of images per class\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "#####################################\n",
    "######### Show sample of images.\n",
    "#####################################\n",
    "plt.figure(figsize=(20, 16))\n",
    "images = []\n",
    "labels = []\n",
    "for itr in train_ds.next():\n",
    "    for i in range(30):\n",
    "        if len(images) < 30:\n",
    "            images.append(itr[i].astype(\"uint8\"))\n",
    "        else:\n",
    "            labels.append(list(itr[i]).index(1))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    ax = plt.subplot(5, 6, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(class_names[labels[i]].replace('_',' '))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the model\n",
    "\n",
    "\n",
    "We now design the architecture for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dhx9ZVzlV9CE"
   },
   "outputs": [],
   "source": [
    "# Defining your model here:\n",
    "model = models.Sequential()\n",
    "model.add(keras.Input(shape=image_size + (3,))) \n",
    "model.add(layers.experimental.preprocessing.Rescaling(1./255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 1</b></span>\n",
    "\n",
    "\n",
    "Define a convolutional neural network (CNN) architecture as follows:\n",
    "\n",
    "1. CNN Block 1:\n",
    "    - Add a dropout layer with a dropout rate of 0.25. Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training, which helps prevent overfitting.\n",
    "    - Add a convolutional layer with 16 filters, each having a 5x5 receptive field. The activation function used is ReLU (Rectified Linear Unit), and \"same\" padding ensures that the output size remains the same as the input size.\n",
    "    - Add a max-pooling layer with a 2x2 pool size. Max-pooling reduces the spatial dimensions of the previous layer's output, helping to retain important features while reducing computational complexity.\n",
    "\n",
    "2. CNN Block 2:\n",
    "    - Add another dropout layer with a dropout rate of 0.25. \n",
    "    - Similar to the first convolutional layer, add another convolutional layer with 32 filters, each having a 5x5 receptive field and ReLU activation.\n",
    "    - Add another max-pooling layer  after the second convolutional layer with a 2x2 pool size.\n",
    "\n",
    "3. CNN Block 3:\n",
    "    - Add another dropout layer with a dropout rate of 0.25.\n",
    "\n",
    "    - Add a third convolutional layer with 32 filters, each having a 5x5 receptive field and ReLU activation.\n",
    "\n",
    "    - Add another max-pooling layer after the third convolutional layer with a 2x2 pool size.\n",
    "\n",
    "4. CNN Block 4:    \n",
    "    - Add another dropout layer with a dropout rate of 0.25.\n",
    "    - Add a fourth convolutional layer with 32 filters, each having a 5x5 receptive field and ReLU activation.\n",
    "    - Add another max-pooling layer after the third convolutional layer with a 2x2 pool size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START YOU CODE HERE\n",
    "pass\n",
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "#Dense part\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model by defininf an optimizer, a loss function, \n",
    "# and the metrics to be used for monitoring the traning.\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning\n",
    "\n",
    "Let's now starting the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtOTspgk9k8r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the trining by defining the number of epochs to train, the traing set and the validation set.\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=50, \n",
    "    validation_data=val_ds,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and analysis of the model\n",
    "\n",
    "The next step consists of monitoring the traning process to investigate possible overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ObvEJffXxiC"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.fill_between(epochs, loss,val_loss,color='g',alpha=.1)\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.fill_between(epochs, acc,val_acc,color='g',alpha=.1)\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `classification_report` function below to build a text report showing the main classification metrics for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.reset()\n",
    "val_ds.shuffle = False\n",
    "val_ds.next()\n",
    "y_prob = model.predict(val_ds)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = val_ds.labels\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the  next cell to create a confusion matrix function `plot_confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozFeX6Xu9k8w"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    vmax = cm.max()\n",
    "    if normalize:\n",
    "        title = 'Confusion matrix (normalized)'\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = [[int(j*100) for j in i ] for i in cm]\n",
    "        cm =np.array(cm)\n",
    "        vmax = 100\n",
    "        \n",
    "    plt.figure(figsize=(14,14))\n",
    "\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=vmax)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar(im,fraction=0.046, pad=0.04)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc5KeKBT9k8y"
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm=cnf_matrix, classes=class_names, title='Confusion Matrix', normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 2</b></span>\n",
    "\n",
    "\n",
    "Examine the confusion matrix and provide an observation-based description of its content and patterns.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's now have a look at miss-classified examples by your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.reset()\n",
    "val_ds.shuffle = True\n",
    "#####################################\n",
    "######### Show sample of images.\n",
    "#####################################\n",
    "plt.figure(figsize=(16, 15))\n",
    "images = []\n",
    "labels_pred = []\n",
    "labels_true = []\n",
    "while len(images) < 20: \n",
    "    batch , labels = val_ds.next()\n",
    "    for i in range(batch.shape[0]): \n",
    "        y_prob_ = model.predict(np.array([batch[i]]), verbose=0)\n",
    "        y_pred_ = np.argmax(y_prob_, axis=1)[0]\n",
    "        y_true_ = list(labels[i]).index(1)\n",
    "        if y_true_!=y_pred_:\n",
    "            images.append(batch[i].astype(\"uint8\"))\n",
    "            labels_pred.append(y_pred_)\n",
    "            labels_true.append(y_true_)\n",
    "            \n",
    "for i in range(20):\n",
    "    if labels_pred[i] != labels_true[i]:\n",
    "        ax = plt.subplot(4, 5, i + 1)\n",
    "        plt.imshow(images[i])    \n",
    "        title = 'Pred: ' + class_names[labels_pred[i]].replace('_',' ') +'\\n' +'True: ' + class_names[labels_true[i]].replace('_',' ') \n",
    "\n",
    "\n",
    "        plt.title(title,fontsize= 11, pad=5)\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 3</b></span>\n",
    "\n",
    "Take a moment to relect on the mistakes your model may generate and consider the potential risks these errors pose in a critical application such as an autonomous driving vehicle.\n",
    "\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte carlo Dropout\n",
    "\n",
    "Monte Carlo Dropout is a technique used in deep learning to estimate uncertainty in predictions made by a neural network. Regular dropout is a technique where during training, random neurons are \"dropped out\" by setting their outputs to zero. This prevents the network from becoming overly reliant on specific neurons and helps with generalization.\n",
    "\n",
    "In the context of Monte Carlo Dropout, dropout is applied not only during training but also during inference (when making predictions). Instead of using a single forward pass to make a prediction, Monte Carlo Dropout involves performing multiple forward passes with dropout enabled. By averaging the predictions from these multiple passes, you can obtain a more robust estimate of uncertainty associated with each prediction.\n",
    "\n",
    "Monte Carlo Dropout provides a way to model and quantify uncertainty in neural network predictions. It's particularly useful in tasks like Bayesian deep learning and uncertainty estimation for tasks such as image classification, where knowing the uncertainty of each prediction can be beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.reset()\n",
    "val_ds.shuffle = False\n",
    "# Initialize an empty list to store batches of data\n",
    "X_val = []\n",
    "\n",
    "# Iterate through all batches of data\n",
    "for batch in val_ds:\n",
    "    # Append the current batch to the list\n",
    "    X_val.append(batch[0])  # Assuming batch[0] contains the data\n",
    "    # Break the loop when all batches have been processed\n",
    "    if val_ds.batch_index == 0:\n",
    "        break\n",
    "\n",
    "X_val = np.concatenate(X_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 4</b></span>\n",
    "\n",
    "Iterate through the specified number of predictions (200) and utilize dropout when making predictions with the trained model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store predictions\n",
    "mote_carlo_pred = []\n",
    "# Loop through the desired number of predictions\n",
    "for _ in trange(200, ncols=100):\n",
    "    ## START YOU CODE HERE\n",
    "    pass\n",
    "    ## END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 5</b></span>\n",
    "\n",
    "In our approach, we leverage the concept of entropy as a fundamental tool to estimate uncertainty. By measuring the level of unpredictability within a given set of predictions, entropy provides a quantifiable metric that helps us gauge the uncertainty associated with each prediction. This utilization of entropy enables us to make more informed decisions and predictions in scenarios where uncertainty plays a pivotal role, fostering a deeper understanding of the underlying data dynamics and enhancing the reliability of our outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(vector):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a given vector.\n",
    "\n",
    "    Parameters:\n",
    "        vector (list or numpy array): Input data vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed entropy value.\n",
    "    \"\"\"\n",
    "    ## START YOU CODE HERE\n",
    "    pass\n",
    "    ## END\n",
    "    return entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 6</b></span>\n",
    "\n",
    "Find the most frequent prediction for each set of predictions in mote_carlo_pred and calculate the entropy of each set of predictions, storing both the most frequent predictions and the calculated entropies in separate lists (y_pred_most_freq and entropy, respectively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_most_freq = []\n",
    "entropy = []\n",
    "for data in mote_carlo_pred:\n",
    "    ## START YOU CODE HERE\n",
    "    pass\n",
    "    ## END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to contrast the accuracy achieved by our model with that obtained through the utilization of the most frequently predicted value employing Monte Carlo dropout.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of our model:', accuracy_score(y_pred, y_true))\n",
    "print('Accuracy utilizing the most frequently predicted value employing Monte Carlo dropout:', \n",
    "      accuracy_score(y_pred_most_freq, y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 7</b></span>\n",
    "\n",
    "\n",
    "What do you conclude from the obtained performance?\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now organize information about predictions, whether they are correct or incorrect, their associated entropy values, and their confidence scores into a Pandas DataFrame for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = np.max(y_prob, axis=1)\n",
    "data = {'Prediction': np.array(['Correct' if x == y else 'Incorrect' for x, y in zip(y_pred, y_true)]), \n",
    "        'Entropy value': entropy,\n",
    "        'Confidence score': confidence_scores}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's employ boxplots of the entropy and confidence score distributions to illustrate the distinction between accurate and erroneous predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Box plot for 'Entropy value'\n",
    "sns.boxplot(data=df, \n",
    "            x='Prediction', \n",
    "            y='Entropy value', \n",
    "            hue='Prediction', \n",
    "            order=[\"Correct\", \"Incorrect\"],\n",
    "            notch=True, \n",
    "            showcaps=True,\n",
    "            flierprops={\"marker\": \"x\"},\n",
    "            ax=axes[0])  # Put it in the first subplot (axes[0])\n",
    "\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set(xlabel='')\n",
    "axes[0].set(ylim=(0, None))\n",
    "axes[0].set_title('Box Plot for Entropy Value')\n",
    "\n",
    "# Box plot for 'Confidence score'\n",
    "sns.boxplot(data=df, \n",
    "            x='Prediction', \n",
    "            y='Confidence score', \n",
    "            hue='Prediction', \n",
    "            order=[\"Correct\", \"Incorrect\"],\n",
    "            notch=True, \n",
    "            showcaps=True,\n",
    "            flierprops={\"marker\": \"x\"},\n",
    "            ax=axes[1])  # Put it in the second subplot (axes[1])\n",
    "\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set(xlabel='')\n",
    "axes[1].set(ylim=(0, None))\n",
    "axes[1].set_title('Box Plot for Confidence Score')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 8</b></span>\n",
    "\n",
    "What do you conclude from the obtained results?\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to visualize the achieved accuracy against the proportion of predicted examples from the validation set, considering different entropy cutoff values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "size = []\n",
    "for v in [x * 0.01 for x in range(1, 410)]:\n",
    "    df2 = df[(df['Entropy value']<v)]\n",
    "    count = len(df2[df2['Prediction'] == 'Correct'])\n",
    "    accuracy.append(count/len(df2))\n",
    "    size.append(len(df2)/len(df))\n",
    "# Create a line plot\n",
    "plt.plot(size, accuracy, marker='o', linestyle='-')\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Size')\n",
    "\n",
    "# Set a title for the plot\n",
    "plt.title('Accuracy vs. Size')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)  # Add grid lines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 9</b></span>\n",
    "\n",
    "What do you conclude from the obtained results? \n",
    "\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><b>TASK 10</b></span>\n",
    "\n",
    "Could you propose an alternative technique that harnesses uncertainty to enhance the model's overall performance?\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "PLEASE ANSWER HERE!\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgment\n",
    "\n",
    "Please feel free contact me if you identify errors, bugs, or issue with this assignment.\n",
    "\n",
    "**Author:** [Mohamed Reda Bouadjenek](https://rbouadjenek.github.io/), Lecturer of Applied Artificial Intelligence, \n",
    "\n",
    "**Institution:** Deakin University, School of Information Technology, Faculty of Sci Eng & Built Env\n",
    "\n",
    "**Adress:** Locked Bag 20000, Geelong, VIC 3220\n",
    "\n",
    "**Phone:** +61 3 522 78380\n",
    "\n",
    "**Email:** reda.bouadjenek@deakin.edu.au\n",
    "\n",
    "**www.deakin.edu.au**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deakin_ai_challenge_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
